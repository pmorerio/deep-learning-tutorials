{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "The main difference between CNNs and standard DNNs is that neurons are arranged in 3 dimensions, **width, height, and depth**.  Each layer in a CNN takes in an input 3d tensor and output a 3d tensor.\n",
    "\n",
    "\n",
    "There are 3 main layers in typical CNN:\n",
    "- Convolutional Layer\n",
    "- Pooling Layer\n",
    "- Fully-Connected Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layer\n",
    "\n",
    "Convolutional kernels are nothing new to image processing.  This kernels are small matricies that we use for blurring, sharpening, edge detection, and more.\n",
    "\n",
    "$\n",
    "\\text{Identity: } \\quad \n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{Blur: } \\qquad  \n",
    "\\begin{bmatrix}\n",
    "1/9 & 1/9 & 1/9 \\\\\n",
    "1/9 & 1/9 & 1/9 \\\\\n",
    "1/9 & 1/9 & 1/9\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{Sobel (edge): }  \n",
    "\\begin{bmatrix}\n",
    "-1 & 0 & 1 \\\\\n",
    "-2 & 0 & 2 \\\\\n",
    "-1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "By convolving these kernels across the image we can isolate certain characteristics, or **features**.  A large number at a location after the convolution indicates a large **activation** or response to the kernel.  In the case of the sobel filter a high activation indicates there is an edge present.  This site is an awesome visualization of kernels: http://setosa.io/ev/image-kernels/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs use filters like above for identifying features in 2D space.  However, the magic with CNNs is that these filters are not predetermined, but rather they are learned.  Also these leaned filters are stacked to look for complex, high level features.  For example, instead of just detecting edges we can now detect cars and tires. \n",
    "\n",
    "<img  src=\"images/cnn_filters.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These filters are small spacially, meaning along the width and height, but always extend to the full depth of the input volume. For example 5x5x3 for a 224x224x3 image.\n",
    "\n",
    "#### Hyperparameters\n",
    " - W - Input volume size\n",
    " - F - Receptive field size\n",
    " - S - Stride\n",
    " - P - Amount of 0 padding used\n",
    " \n",
    " \n",
    "Output size = $\\frac{(W-F+2 P)}{S}+1$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer\n",
    "​\n",
    "The function of this layer is to reduce the spacial size of the representation. The benefit of this is three fold: reduce the amount of parameters, lower computation time, and control overfitting.  There are variants like max, average, or overlapping.\n",
    "\n",
    "​\n",
    "<img  src=\"./images/maxpool.jpeg\"/>\n",
    "​\n",
    "\n",
    "It is worth noting that while this has been standard, a few recent publications have abandoned it.  It could be falling out of favor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layer\n",
    "\n",
    "A fully connected layer takes the activations and flattens them to behave like a normal layer in a DNN.  Now with all these pieces we can create a whole CNN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "#from utils import load_CIFAR10\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data\n",
    "mnist_train, mnist_test = tf.keras.datasets.mnist.load_data(\n",
    "    path='mnist.npz'\n",
    ")\n",
    "\n",
    "mnist_train_images, mnist_train_labels = mnist_train\n",
    "mnist_test_images, mnist_test_labels = mnist_test\n",
    "\n",
    "from keras.utils.np_utils import to_categorical   \n",
    "\n",
    "mnist_train_labels = to_categorical(mnist_train_labels)\n",
    "mnist_test_labels = to_categorical(mnist_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist_train_images[0])\n",
    "print(mnist_train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.random.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step [0]: training accuracy 0.0800\n",
      "step [100]: training accuracy 0.8900\n",
      "step [200]: training accuracy 0.9000\n",
      "step [300]: training accuracy 0.9100\n",
      "step [400]: training accuracy 0.9500\n",
      "step [500]: training accuracy 0.9600\n",
      "step [600]: training accuracy 0.9800\n",
      "step [700]: training accuracy 0.9600\n",
      "step [800]: training accuracy 0.9400\n",
      "step [900]: training accuracy 0.9800\n",
      "step [1000]: training accuracy 0.9700\n",
      "step [1100]: training accuracy 0.9900\n",
      "step [1200]: training accuracy 0.9800\n",
      "step [1300]: training accuracy 0.9600\n",
      "step [1400]: training accuracy 0.9600\n",
      "step [1500]: training accuracy 0.9800\n",
      "step [1600]: training accuracy 0.9800\n",
      "step [1700]: training accuracy 1.0000\n",
      "step [1800]: training accuracy 0.9800\n",
      "step [1900]: training accuracy 0.9700\n",
      "step [2000]: training accuracy 0.9600\n",
      "step [2100]: training accuracy 0.9800\n",
      "step [2200]: training accuracy 0.9700\n",
      "step [2300]: training accuracy 1.0000\n",
      "step [2400]: training accuracy 0.9900\n",
      "step [2500]: training accuracy 0.9800\n",
      "step [2600]: training accuracy 0.9600\n",
      "step [2700]: training accuracy 0.9800\n",
      "step [2800]: training accuracy 0.9700\n",
      "step [2900]: training accuracy 1.0000\n",
      "step [3000]: training accuracy 0.9900\n",
      "step [3100]: training accuracy 0.9900\n",
      "step [3200]: training accuracy 0.9900\n",
      "step [3300]: training accuracy 0.9900\n",
      "step [3400]: training accuracy 0.9900\n",
      "step [3500]: training accuracy 1.0000\n",
      "step [3600]: training accuracy 0.9900\n",
      "step [3700]: training accuracy 1.0000\n",
      "step [3800]: training accuracy 0.9800\n",
      "step [3900]: training accuracy 0.9900\n",
      "step [4000]: training accuracy 0.9900\n",
      "step [4100]: training accuracy 1.0000\n",
      "step [4200]: training accuracy 0.9900\n",
      "step [4300]: training accuracy 1.0000\n",
      "step [4400]: training accuracy 0.9900\n",
      "step [4500]: training accuracy 1.0000\n",
      "step [4600]: training accuracy 0.9900\n",
      "step [4700]: training accuracy 1.0000\n",
      "step [4800]: training accuracy 0.9900\n",
      "step [4900]: training accuracy 1.0000\n",
      "--- RESULTS ---\n",
      "Test Accuracy: 0.9700\n",
      "CPU times: user 10.4 s, sys: 938 ms, total: 11.3 s\n",
      "Wall time: 4.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# flatten to a vector\n",
    "l_train = len(mnist_train_labels)\n",
    "l_test = len(mnist_test_labels)\n",
    "\n",
    "train_images = np.reshape(mnist_train_images, [l_train,784])/255.\n",
    "test_images = np.reshape(mnist_test_images, [l_test,784])/255.\n",
    "\n",
    "# Placeholders\n",
    "y_true = tf.compat.v1.placeholder(tf.float32, [None, 10])\n",
    "x = tf.compat.v1.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# Layer 1\n",
    "W1 = weight_variable([784, 100])\n",
    "b1 = bias_variable([100])\n",
    "h1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "\n",
    "# Layer 2\n",
    "W2 = weight_variable([100, 10])\n",
    "b2 = bias_variable([10])\n",
    "y = tf.matmul(h1, W2) + b2\n",
    "\n",
    "# Define some operations: loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_true))\n",
    "train_step = tf.compat.v1.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_true, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Start session\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# Train\n",
    "for i in range(5000):\n",
    "    batch_xs, batch_ys = train_images[i*batch_size % l_train : (i+1)*batch_size  % l_train],\\\n",
    "                        mnist_train_labels[i*batch_size % l_train : (i+1)*batch_size % l_train]\n",
    "    if i % 100 == 0:\n",
    "        _, acc = sess.run([train_step, accuracy], feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "        print(\"step [%d]: training accuracy %.4f\" % (i, acc))\n",
    "    else:\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "        \n",
    "\n",
    "# Test trained model\n",
    "print(\"--- RESULTS ---\")\n",
    "print(\"Test Accuracy: %.4f\" % sess.run(accuracy, feed_dict={x: test_images[:1000],\n",
    "                              y_true: mnist_test_labels[:1000]}))\n",
    "sess.close()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla CNN\n",
    "\n",
    "This is the most basic CNN.  It has 2 convolutional layers and 2 fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    # [batch, in_height, in_width, in_channels]\n",
    "    # [filter_height, filter_width, in_channels, out_channels]\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step [0]: training accuracy 0.1100\n",
      "step [100]: training accuracy 0.8600\n",
      "step [200]: training accuracy 0.9000\n",
      "step [300]: training accuracy 0.9300\n",
      "step [400]: training accuracy 0.9600\n",
      "step [500]: training accuracy 0.9700\n",
      "step [600]: training accuracy 0.9400\n",
      "step [700]: training accuracy 0.9400\n",
      "step [800]: training accuracy 0.9600\n",
      "step [900]: training accuracy 0.9700\n",
      "step [1000]: training accuracy 0.9900\n",
      "step [1100]: training accuracy 0.9900\n",
      "step [1200]: training accuracy 0.9900\n",
      "step [1300]: training accuracy 1.0000\n",
      "step [1400]: training accuracy 0.9700\n",
      "step [1500]: training accuracy 0.9700\n",
      "step [1600]: training accuracy 1.0000\n",
      "step [1700]: training accuracy 1.0000\n",
      "step [1800]: training accuracy 1.0000\n",
      "step [1900]: training accuracy 0.9900\n",
      "--- RESULTS ---\n",
      "Test Accuracy 0.9800\n",
      "CPU times: user 2h 38s, sys: 6min 23s, total: 2h 7min 2s\n",
      "Wall time: 12min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.compat.v1.reset_default_graph()\n",
    "# Placeholders\n",
    "y_true = tf.compat.v1.placeholder(tf.float32, [None, 10])\n",
    "x = tf.compat.v1.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# Resize to appropriate input tensor:\n",
    "# [batch, in_height, in_width, in_channels]\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "# First Convolutional Layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "# Second Convolutional Layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2) + b_conv2)\n",
    "\n",
    "# Densely Connected Layer\n",
    "W_fc1 = weight_variable([28*28*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_conv2_flat = tf.reshape(h_conv2, [-1, 28*28*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_conv2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Readout Layer\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "\n",
    "# Loss\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_true))\n",
    "train_step = tf.compat.v1.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_true,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "for i in range(2000):\n",
    "    batch_xs, batch_ys = train_images[i*batch_size % l_train : (i+1)*batch_size  % l_train],\\\n",
    "                        mnist_train_labels[i*batch_size % l_train : (i+1)*batch_size % l_train]\n",
    "    if i % 100 == 0:\n",
    "        _, acc = sess.run([train_step, accuracy], feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "        print(\"step [%d]: training accuracy %.4f\" % (i, acc))\n",
    "    else:\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "        \n",
    "print(\"--- RESULTS ---\")\n",
    "print(\"Test Accuracy %.4f\" % sess.run(accuracy, feed_dict={\n",
    "    x: test_images[:1000], y_true: mnist_test_labels[:1000]}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + MaxPool + Dropout\n",
    "\n",
    "This CNN is the same as above plus max pooling and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step [0]: training accuracy 0.0800\n",
      "step [100]: training accuracy 0.7600\n",
      "step [200]: training accuracy 0.8300\n",
      "step [300]: training accuracy 0.8800\n",
      "step [400]: training accuracy 0.9500\n",
      "step [500]: training accuracy 0.9700\n",
      "step [600]: training accuracy 0.9200\n",
      "step [700]: training accuracy 0.9800\n",
      "step [800]: training accuracy 0.9100\n",
      "step [900]: training accuracy 0.9600\n",
      "step [1000]: training accuracy 0.9700\n",
      "step [1100]: training accuracy 1.0000\n",
      "step [1200]: training accuracy 0.9600\n",
      "step [1300]: training accuracy 0.9900\n",
      "step [1400]: training accuracy 0.9700\n",
      "step [1500]: training accuracy 0.9700\n",
      "step [1600]: training accuracy 0.9900\n",
      "step [1700]: training accuracy 0.9700\n",
      "step [1800]: training accuracy 0.9900\n",
      "step [1900]: training accuracy 1.0000\n",
      "--- RESULTS ---\n",
      "Test Accuracy 0.9820\n",
      "CPU times: user 18min 16s, sys: 20.6 s, total: 18min 36s\n",
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "y_true = tf.compat.v1.placeholder(tf.float32, [None, 10])\n",
    "x = tf.compat.v1.placeholder(tf.float32, [None, 784])\n",
    "drop_prob = tf.compat.v1.placeholder(tf.float32)\n",
    "\n",
    "# Resize to appropriate input tensor:\n",
    "# [batch, in_height, in_width, in_channels]\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "# First Convolutional Layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1) # <------------------------- Added Pooling\n",
    "\n",
    "# Second Convolutional Layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2) # <------------------------- Added Pooling\n",
    "\n",
    "# Densely Connected Layer\n",
    "W_fc1 = weight_variable([7*7*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, drop_prob) # <------------ Added Dropout\n",
    "\n",
    "# Readout Layer\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "# Loss\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_true))\n",
    "train_step = tf.compat.v1.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_true,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "for i in range(2000):\n",
    "    batch_xs, batch_ys = train_images[i*batch_size % l_train : (i+1)*batch_size  % l_train],\\\n",
    "                        mnist_train_labels[i*batch_size % l_train : (i+1)*batch_size % l_train]\n",
    "    if i % 100 == 0:\n",
    "        _, acc = sess.run([train_step, accuracy], feed_dict={x: batch_xs, y_true: batch_ys, drop_prob: 0.2})\n",
    "        print(\"step [%d]: training accuracy %.4f\" % (i, acc))\n",
    "    else:\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y_true: batch_ys, drop_prob: 0.2})\n",
    "        \n",
    "print(\"--- RESULTS ---\")\n",
    "print(\"Test Accuracy %.4f\" % sess.run(accuracy, feed_dict={\n",
    "    x: test_images[:1000], y_true: mnist_test_labels[:1000], drop_prob: 0.}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CNN Architectures\n",
    "\n",
    "<img src=\"./images/full_net.png\">\n",
    "\n",
    "The most common architecture follow the following pattern.\n",
    "\n",
    "INPUT - > [[CONV -> RELU]xN -> POOL?]xM -> [FC -. RELU]xk -> FC\n",
    "\n",
    "<img src=\"./images/vgg.png\">\n",
    "\n",
    "There are several standard architectures that are legendary (standard and influential).\n",
    "\n",
    "#### AlexNet\n",
    "- Trained the network on ImageNet data, which contained over 15 million annotated images from a total of over 22,000 categories.\n",
    "- Used ReLU for the nonlinearity functions (Found to decrease training time as ReLUs are several times faster than the conventional tanh function).\n",
    "- Used data augmentation techniques that consisted of image translations, horizontal reflections, and patch extractions.\n",
    "- Implemented dropout layers in order to combat the problem of overfitting to the training data.\n",
    "- Trained the model using batch stochastic gradient descent, with specific values for momentum and weight decay.\n",
    "- Trained on two GTX 580 GPUs for five to six days.\n",
    "\n",
    "#### ZF Net\n",
    "- Very similar architecture to AlexNet, except for a few minor modifications.\n",
    "- AlexNet trained on 15 million images, while ZF Net trained on only 1.3 million images.\n",
    "- Instead of using 11x11 sized filters in the first layer (which is what AlexNet implemented), ZF Net used filters of size 7x7 and a decreased stride value. The reasoning behind this modification is that a smaller filter size in the first conv layer helps retain a lot of original pixel information in the input volume. A filtering of size 11x11 proved to be skipping a lot of relevant information, especially as this is the first conv layer.\n",
    "- As the network grows, we also see a rise in the number of filters used.\n",
    "- Used ReLUs for their activation functions, cross-entropy loss for the error function, and trained using batch stochastic gradient descent.\n",
    "- Trained on a GTX 580 GPU for twelve days.\n",
    "- Developed a visualization technique named Deconvolutional Network, which helps to examine different feature activations and their relation to the input space. Called “deconvnet” because it maps features to pixels (the opposite of what a convolutional layer does).\n",
    "\n",
    "#### VGG Net\n",
    "- The use of only 3x3 sized filters is quite different from AlexNet’s 11x11 filters in the first layer and ZF Net’s 7x7 filters. The authors’ reasoning is that the combination of two 3x3 conv layers has an effective receptive field of 5x5. This in turn simulates a larger filter while keeping the benefits of smaller filter sizes. One of the benefits is a decrease in the number of parameters. Also, with two conv layers, we’re able to use two ReLU layers instead of one.\n",
    "- 3 conv layers back to back have an effective receptive field of 7x7.\n",
    "- As the spatial size of the input volumes at each layer decrease (result of the conv and pool layers), the depth of the volumes increase due to the increased number of filters as you go down the network.\n",
    "- Interesting to notice that the number of filters doubles after each maxpool layer. This reinforces the idea of shrinking spatial dimensions, but growing depth.\n",
    "- Worked well on both image classification and localization tasks. The authors used a form of localization as regression (see page 10 of the paper for all details).\n",
    "- Built model with the Caffe toolbox.\n",
    "- Used scale jittering as one data augmentation technique during training.\n",
    "- Used ReLU layers after each conv layer and trained with batch gradient descent.\n",
    "- Trained on 4 Nvidia Titan Black GPUs for two to three weeks.\n",
    "\n",
    "#### GoogLeNet\n",
    "- Used 9 Inception modules in the whole architecture, with over 100 layers in total! Now that is deep…\n",
    "- No use of fully connected layers! They use an average pool instead, to go from a 7x7x1024 volume to a 1x1x1024 volume. This saves a huge number of parameters.\n",
    "- Uses 12x fewer parameters than AlexNet.\n",
    "- During testing, multiple crops of the same image were created, fed into the network, and the softmax probabilities were averaged to give us the final solution.\n",
    "- Utilized concepts from R-CNN (a paper we’ll discuss later) for their detection model.\n",
    "- There are updated versions to the Inception module (Versions 6 and 7).\n",
    "- Trained on “a few high-end GPUs within a week”.\n",
    "\n",
    "#### ResNet\n",
    "- “Ultra-deep” – Yann LeCun.\n",
    "- 152 layers…\n",
    "- Interesting note that after only the first 2 layers, the spatial size gets compressed from an input volume of 224x224 to a 56x56 volume.\n",
    "- Authors claim that a naïve increase of layers in plain nets result in higher training and test error (Figure 1 in the paper).\n",
    "- The group tried a 1202-layer network, but got a lower test accuracy, presumably due to overfitting.\n",
    "- Trained on an 8 GPU machine for two to three weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Links:\n",
    "Interactive Demo:\n",
    "- http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html\n",
    "- http://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html\n",
    "\n",
    "Visualizing Activations:\n",
    "- https://www.youtube.com/watch?v=AgkfIQ4IGaM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "[1] http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "[2] https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ddn_tutorial)",
   "language": "python",
   "name": "ddn_tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
